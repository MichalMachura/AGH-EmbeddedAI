{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN pruning and quantization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization allows for decreasing data memory size necessary to store them.\n",
    "\n",
    "Also allows for faster computation with dedicated devices like embedded GPU and FPGA.\n",
    "\n",
    "Applying quantization on continues data creates regions of attraction.\n",
    "\n",
    "Values from some range are assigned to proper value that represents that range.\n",
    "\n",
    "That also increase the level of correlation between quantized filters, \n",
    "\n",
    "so allows for more effective pruning.\n",
    "\n",
    "That results with much more smaller network than at the beginning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "from copy import deepcopy\n",
    "\n",
    "# local_utils file contains util functions for training or display\n",
    "# CHECK IT\n",
    "import local_utils as lu\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "def set_random_seed(seed:int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader) = 938\n",
      "len(test_loader) = 157\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABxsAAADdCAYAAABjXhx/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAudElEQVR4nO3de5SdVX038N8OCRhEwrUYLxgQVBAhXEXeLKDlakRuVoRytZawRAJ0CYUixVhEEYF3BRA0cgkgr+gqQtBCgQoEFaREiparCCVyCTchJsRAhHnePzK2E3z2ycxz5szZk/l81soi2d/Zz/ObId+57ZwzqaqqAAAAAAAAABioUd0eAAAAAAAAABieHDYCAAAAAAAAjThsBAAAAAAAABpx2AgAAAAAAAA04rARAAAAAAAAaMRhIwAAAAAAANDI6HY2p5T2jIjpEbFSRFxcVdWZy3n5qp37wTD1YlVV63b6JvoI/aKPUA59hHLoI5RDH6Ec+gjl0EcoR20fGz+yMaW0UkR8IyI+GhGbRsRBKaVNm88HK6y5nb6BPkK/6SOUQx+hHPoI5dBHKIc+Qjn0EcpR28d2nkZ1u4j4TVVVj1dVtSQiro6Ifdq4HtCcPkI59BHKoY9QDn2EcugjlEMfoRz6CG1o57DxnRHxZJ8/P9W7toyU0pSU0pyU0pw27gW0po9QDn2EcugjlEMfoRz6COXQRyiHPkIb2vqZjf1RVdWMiJgR4TmModv0Ecqhj1AOfYRy6COUQx+hHPoI5dBHqNfOIxufjoh39/nzu3rXgKGnj1AOfYRy6COUQx+hHPoI5dBHKIc+QhvaOWy8JyI2TiltkFJaOSIOjIjrB2csYID0Ecqhj1AOfYRy6COUQx+hHPoI5dBHaEPjp1Gtqur1lNIxEXFTRKwUEZdWVfXAoE0G9Js+Qjn0Ecqhj1AOfYRy6COUQx+hHPoI7UlVNXRPK+w5jBmhflFV1TbdHuLN9JERSh+hHPoI5dBHKIc+Qjn0Ecqhj1CO2j628zSqAAAAAAAAwAjmsBEAAAAAAABoxGEjAAAAAAAA0IjDRgAAAAAAAKARh40AAAAAAABAIw4bAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADTisBEAAAAAAABoxGEjAAAAAAAA0IjDRgAAAAAAAKARh40AAAAAAABAIw4bAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADTisBEAAAAAAABoZHS3BwDgf2299dbZ7JhjjqldP+yww7J7rrjiimx2/vnnZ7N77703mwEAAAAMV9OnT89mxx57bDa7//77s9lee+2VzebOndu/wQCGMY9sBAAAAAAAABpx2AgAAAAAAAA04rARAAAAAAAAaMRhIwAAAAAAANCIw0YAAAAAAACgEYeNAAAAAAAAQCOj29mcUnoiIhZGxBsR8XpVVdsMxlDkrbTSStls3Lhxg36/Y445pnZ91VVXze55//vfn80+97nPZbOzzz67dv2ggw7K7nn11Vez2ZlnnpnNvvSlL2Wz4Uofh4+JEydms1tuuSWbrb766rXrVVVl9xx66KHZbO+9985ma6+9djZj+fSRwbTLLrvUrl911VXZPTvttFM2e+SRR9qeaTjRR+qceuqp2azV54mjRtX/W82dd945u2f27Nn9nmtFp49QDn1cMbztbW/LZquttlrt+sc+9rHsnnXXXTebnXvuudnstddey2Ysnz52zoQJE2rXDznkkOyenp6ebLbJJptksw984APZbO7cudmMsuhj57zvfe+rXR8zZkx2z4477pjNLrzwwmzWqsdDadasWbXrBx54YHbPkiVLOjVOx7V12NjrL6uqenEQrgO0Tx+hHPoI5dBHKIc+Qjn0Ecqhj1AOfYQGPI0qAAAAAAAA0Ei7h41VRNycUvpFSmlK3QuklKaklOaklOa0eS+gNX2EcugjlEMfoRz6COXQRyiHPkI59BEaavdpVCdVVfV0SukvIuKWlNLDVVXd0fcFqqqaEREzIiJSSvkfLAa0Sx+hHPoI5dBHKIc+Qjn0Ecqhj1AOfYSG2npkY1VVT/f+9/mIuDYithuMoYCB00cohz5COfQRyqGPUA59hHLoI5RDH6G5xoeNKaW3ppTe9qffR8TuEXH/YA0G9J8+Qjn0Ecqhj1AOfYRy6COUQx+hHPoI7WnnaVTXi4hrU0p/us7/q6rq3wZlqmFm/fXXz2Yrr7xyNtthhx2y2aRJk2rX11hjjeyeT3ziE9lsKD311FPZ7Lzzzstm++23X+36woULs3t++ctfZrPZs2dnsxWQPhZmu+3y//DpmmuuyWbjxo3LZlVV/8wMrTqyZMmSbLb22mtns+233752/d577210rxGm6D7uuOOO2azV34lrr722E+PQD9tuu23t+j333DPEkwxLRfeRzjriiCOy2UknnZTNenp6Bnyv3MdolqGPUA59LMyECROyWauPWR/5yEey2WabbdbOSH9m/Pjx2ezYY48d1HuNMPrYQS+88ELt+h133FG7HhGx9957d2ocyqeP/fDBD34wm7X6GuyTn/xk7fqoUfnHw73jHe/IZq2+bivl67Pc+5NvfvOb2T3HH398NluwYEG7I3VU48PGqqoej4gtBnEWoCF9hHLoI5RDH6Ec+gjl0Ecohz5COfQR2tPWz2wEAAAAAAAARi6HjQAAAAAAAEAjDhsBAAAAAACARhw2AgAAAAAAAI2M7vYAw8XEiROz2a233prNxo0b14Fpuq+npyebnXrqqdnslVdeyWZXXXVV7fq8efOye15++eVs9sgjj2QzGIhVV121dn2rrbbK7vnOd76TzcaPH9/2TH09+uij2eyss87KZldffXU2+9nPfla73qrfX/3qV7MZ5dh5552z2cYbb5zNrr322g5Mw5+MGpX/918bbLBB7fp73vOe7J6UUtszwXDXqiNvectbhnAS6J4Pf/jD2eyQQw7JZjvttFM2++AHPzjgOU444YRs9swzz2SzSZMmZbPc59t33313/weDDvjABz5Qu3788cdn9xx88MHZbOzYsdms1ed8Tz75ZO36woULs3s22WSTbHbAAQdkswsvvLB2/eGHH87ugaGwaNGi2vW5c+cO8SSw4mj1/b/JkycP4STD12GHHZbNLrnkkmyW+35tKTyyEQAAAAAAAGjEYSMAAAAAAADQiMNGAAAAAAAAoBGHjQAAAAAAAEAjDhsBAAAAAACARhw2AgAAAAAAAI2M7vYAw8Vvf/vbbPa73/0um40bN64T4wzY3Xffnc3mz5+fzf7yL/+ydn3JkiXZPVdeeWW/54Lh4Fvf+lbt+kEHHTTEk9Tbaqutstlqq62WzWbPnp3Ndt5559r1zTffvN9zUabDDjssm911111DOAl9jR8/PpsdeeSRtevf+c53snsefvjhtmeC4WLXXXetXZ86dWqj67Xqz1577VW7/txzzzW6FwyWT33qU7Xr06dPz+5ZZ511sllKKZvdfvvt2WzdddetXf/617+e3dNKqzly9zrwwAMb3QverNX3c772ta9ls1wf3/a2t7U905s9+uij2WyPPfaoXR8zZkx2T6uPga3eZ7TKoJvWWGON2vUttthiaAeBFcgtt9ySzSZPnjzg6z3//PPZ7JJLLslmo0blH0fX09Mz4Dl22GGHbLbTTjsN+HojkUc2AgAAAAAAAI04bAQAAAAAAAAacdgIAAAAAAAANOKwEQAAAAAAAGjEYSMAAAAAAADQiMNGAAAAAAAAoJHR3R5guHjppZey2YknnpjN9tprr2z2n//5n9nsvPPO699gfdx3333ZbLfddstmixYtymYf/OAHa9ePO+64fs8Fw8HWW2+dzT72sY/VrqeUGt1r9uzZ2eyHP/xhNjv77LNr15955pnsnlbvZ15++eVs9ld/9Ve1601fZ8oxapR/Z1Siiy++eMB7Hn300Q5MAmWaNGlSNrvssstq18eNG9foXl//+tez2dy5cxtdE/pr9Oj8l+jbbLNNNvv2t79du77qqqtm99xxxx3Z7PTTT89mP/3pT7PZKqusUrv+/e9/P7tn9913z2atzJkzp9E+6K/99tsvm/3d3/3dkM3x2GOPZbNW3+t58skna9c32mijtmeC4SL3cXD99dcf9Httu+222ezhhx+uXfe5JcPRRRddlM2uu+66AV/vj3/8YzZ79tlnB3y9plZfffVsdv/992ezd7zjHQO+V6u303D+HNd3HAEAAAAAAIBGHDYCAAAAAAAAjThsBAAAAAAAABpx2AgAAAAAAAA04rARAAAAAAAAaMRhIwAAAAAAANDI6OW9QErp0ojYKyKer6pqs961tSLiexExISKeiIgDqqp6uXNjlu26667LZrfeems2W7hwYTbbYostatc/85nPZPecffbZ2WzRokXZrJUHHnigdn3KlCmNrkd79LE9EydOzGa33HJLNlt99dVr16uqyu658cYbs9lBBx2UzXbaaadsduqpp9auX3zxxdk9L7zwQjb75S9/mc16enpq1z/2sY9l92y11VbZ7N57781mw1XJfdx8882z2XrrrTeEk9Bf48aNG/CeVu+3RpqS+8jgOPzww7PZO97xjgFf7/bbb89mV1xxxYCvx//Sx/Yccsgh2azV53w5rT5WfOpTn8pmCxYsGPC9Wl1z9913b3S9p556Kptdfvnlja45kuhjez75yU8O6vWeeOKJbHbPPfdks5NOOimbPfnkkwOeY5NNNhnwHtqnj93xzDPP1K7PnDkzu2fatGmN7tVq3/z582vXL7jggkb3oj362J7XX389mzX5uFSKPfbYI5utueaag3qvVp/jvvbaa4N6r6HUn0c2zoyIPd+0dnJE/Liqqo0j4se9fwY6b2boI5RiZugjlGJm6COUYmboI5RiZugjlGJm6COUYmboIwy65R42VlV1R0S89KblfSLiT/+M8PKI2HdwxwLq6COUQx+hHPoI5dBHKIc+Qjn0Ecqhj9AZy30a1Yz1qqqa1/v7ZyMi+5xsKaUpEeE5N6Fz9BHKoY9QDn2EcugjlEMfoRz6COXQR2hT08PG/1FVVZVSyv7gsqqqZkTEjIiIVi8HtE8foRz6COXQRyiHPkI59BHKoY9QDn2EZvrzMxvrPJdSGh8R0fvf5wdvJGCA9BHKoY9QDn2EcugjlEMfoRz6COXQR2hT08PG6yPi8N7fHx4RswZnHKABfYRy6COUQx+hHPoI5dBHKIc+Qjn0Edq03KdRTSl9NyJ2joh1UkpPRcQXI+LMiPh+SukzETE3Ig7o5JDD2YIFCxrt+/3vfz/gPUceeWQ2+973vpfNenp6BnwvukMfl+9973tfNjvxxBOz2bhx47LZiy++WLs+b9682vWIiMsvvzybvfLKK9nsX//1XxtlQ2Xs2LHZ7POf/3w2O/jggzsxTleV3MfJkydns1b/D+ms9dbL/siH2GCDDQZ8vaeffrqdcVYoJfeR/ltnnXWy2d/+7d9ms9znsvPnz8/u+fKXv9zvuRgYfVy+008/PZudcsop2ayq8s/SdeGFF9aun3rqqdk9Tb9WbeULX/jCoF7v2GOPzWYvvPDCoN5rRaSP7Wn1PZYpU/I/quvmm2+uXf/Nb36T3fP880P3AJpWn5PSOfpYllYfi6dNmzZ0g9AV+jiyHXjggbXrrT7uD/b30k477bRBvV4plnvYWFXVQZlol0GeBVgOfYRy6COUQx+hHPoI5dBHKIc+Qjn0ETqj6dOoAgAAAAAAACOcw0YAAAAAAACgEYeNAAAAAAAAQCMOGwEAAAAAAIBGRnd7AOpNmzatdn3rrbfO7tlpp52y2a677prNbr755n7PBSVYZZVVstnZZ5+dzSZPnpzNFi5cmM0OO+yw2vU5c+Zk94wdOzabrajWX3/9bo9Ar/e///2N9j3wwAODPAl9tXr/tN5662WzX//617Xrrd5vQakmTJiQza655ppBvdf555+fzW677bZBvRe82WmnnZbNTjnllGy2ZMmSbHbTTTdls5NOOql2ffHixdk9rbzlLW/JZrvvvns2y30+mFLK7vnyl7+czWbNmpXNoNOeeeaZbJb7ns1w8JGPfKTbI0DRRo3KPzanp6dnCCcBWjn44IOz2cknn5zNNtpoo9r1MWPGtD3Tm913332163/84x8H/V4l8MhGAAAAAAAAoBGHjQAAAAAAAEAjDhsBAAAAAACARhw2AgAAAAAAAI04bAQAAAAAAAAacdgIAAAAAAAANDK62wNQb9GiRbXrRx55ZHbPvffem82+/e1vZ7Pbbrstm82ZM6d2/Rvf+EZ2T1VV2QwGw5ZbbpnNJk+e3Oia++yzTzabPXt2o2vCcHPPPfd0e4SirL766tlszz33rF0/5JBDsnt23333RnOcfvrptevz589vdD3oplx3IiI233zzRtf88Y9/XLs+ffr0RteDgVhjjTVq148++ujsnlZfL910003ZbN999+3vWP2y0UYbZbOrrroqm2299dYDvte//Mu/ZLOzzjprwNeDFc2xxx6bzd761rcO6r0+9KEPNdp35513ZrO77rqr6ThQnJ6enmzme54QMWHChGx26KGHZrNdd911UOeYNGlSNhvsri5YsCCbnXzyydnshhtuqF1fvHhx2zOVyCMbAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADTisBEAAAAAAABoxGEjAAAAAAAA0Mjobg/AwDz22GPZ7Igjjshml112WTY79NBDB5y99a1vze654oorstm8efOyGfTXueeem81SStls9uzZjbKRaNSo+n+L0tPTM8STMJTWWmutIbvXFltskc1a9XjXXXetXX/Xu96V3bPyyitns4MPPjib5XoQEbF48eLa9bvvvju757XXXstmo0fnPyX7xS9+kc2gRPvuu282O/PMMxtd86c//Wk2O/zww2vXf//73ze6FwxE7mPMOuus0+h6xx57bDb7i7/4i2z26U9/unZ97733zu7ZbLPNstlqq62WzaqqGnD2ne98J7tn0aJF2QxKteqqq2azTTfdtHb9i1/8YnbP5MmTG83R6vPVJl+7PfPMM9ks934mIuKNN94Y8L0AKFerzxOvv/76bLb++ut3Ypyu+8lPfpLNZsyYMYSTlM0jGwEAAAAAAIBGHDYCAAAAAAAAjThsBAAAAAAAABpx2AgAAAAAAAA04rARAAAAAAAAaMRhIwAAAAAAANDI6OW9QErp0ojYKyKer6pqs961aRFxZES80Ptip1RVdUOnhqR/rr322mz26KOPZrNzzz03m+2yyy6161/5yleye97znvdkszPOOCObPf3009mMpUZaH/faa6/a9YkTJ2b3VFWVza6//vp2Rxoxenp6atdbvX3vu+++Dk1TppL7uHjx4mzW6v/hN7/5zWx2yimntDXTm22++ebZLKWUzV5//fXa9T/84Q/ZPQ8++GA2u/TSS7PZnDlzstns2bNr15977rnsnqeeeiqbjR07Nps9/PDD2YylSu7jimzChAm169dcc82g3+vxxx/PZq16x9AbaX1csmRJ7foLL7xQux4Rse6662az//7v/85mrT6GN/HMM89kswULFmSz8ePHZ7MXX3yxdv2HP/xh/wdj0Iy0PjYxZsyYbLbllltms1Yf63IdafU5eqs+3nXXXdlszz33zGarrrpqNssZPTr/bcL9998/m02fPr12Pfc+ciTSRyiHPran1fdsWmWDbdSo/OPoct/XbCr3PeqIiI9+9KPZ7MYbbxzUOUrXn0c2zoyIus9e/m9VVRN7fykeDI2ZoY9Qipmhj1CKmaGPUIqZoY9Qipmhj1CKmaGPUIqZoY8w6JZ72FhV1R0R8dIQzAIshz5COfQRyqGPUA59hHLoI5RDH6Ec+gid0c7PbDwmpfSrlNKlKaU1cy+UUpqSUpqTUso/FxnQLn2EcugjlEMfoRz6COXQRyiHPkI59BHa0PSw8aKIeG9ETIyIeRFxTu4Fq6qaUVXVNlVVbdPwXkBr+gjl0Ecohz5COfQRyqGPUA59hHLoI7Sp0WFjVVXPVVX1RlVVPRHx7YjYbnDHAvpLH6Ec+gjl0Ecohz5COfQRyqGPUA59hPY1OmxMKY3v88f9IuL+wRkHGCh9hHLoI5RDH6Ec+gjl0Ecohz5COfQR2jd6eS+QUvpuROwcEeuklJ6KiC9GxM4ppYkRUUXEExFxVOdGZDDcf3/+/eMBBxyQzT7+8Y/Xrl922WXZPUcdlf/rsPHGG2ez3XbbLZux1Ejr49ixY2vXV1555eye559/Ppt973vfa3um4WaVVVbJZtOmTRvw9W699dZs9o//+I8Dvt5wVnIfjz766Gw2d+7cbLbDDjt0Ypxav/3tb7PZddddl80eeuih2vWf//zn7Y40KKZMmZLN1l133Wz2+OOPd2KcEaPkPq7ITjrppNr1np6eQb/XmWeeOejXpDNGWh/nz59fu77vvvtm9/zoRz/KZmuttVY2e+yxx7LZrFmzatdnzpyZ3fPSSy9ls6uvvjqbjR8/Ppu12sfQG2l9zGn19eOee+6ZzX7wgx80ut+XvvSl2vVWX0v97Gc/y2at3i+0uuZmm22WzXJafb761a9+NZvlPrdv9Xn9a6+91u+5VgT6OHyMGpV/bE7Tz3N33HHH2vULLrig0fVojz4uX6uzhJ133jmbHXLIIdnspptuql1/9dVX+z3XYPjMZz5Tuz516tQhnWNFtNzDxqqqDqpZvqQDswDLoY9QDn2EcugjlEMfoRz6COXQRyiHPkJnNHoaVQAAAAAAAACHjQAAAAAAAEAjDhsBAAAAAACARhw2AgAAAAAAAI2M7vYAdN/8+fOz2ZVXXlm7fvHFF2f3jB6d/2u14447ZrOdd965dv3222/P7oE3e+2117LZvHnzhnCSobPKKqtks1NPPTWbnXjiidnsqaeeql0/55xzsnteeeWVbEY5vva1r3V7hBXaLrvs0mjfNddcM8iTwOCYOHFiNtt9990H9V6zZs3KZo888sig3gs67e67785m66677hBOktfqa7Oddtopm/X09GSzxx9/vK2ZoB1jxoypXf/Sl76U3dPqa6JWbrzxxmx2/vnn1663+t5Lq/cLN9xwQzb70Ic+lM2WLFlSu37WWWdl92y22WbZbJ999slmV111Ve36v//7v2f3tPq65OWXX85mrdx3332N9kFfrT7OVVXV6Jr7779/7fqmm26a3fPggw82uhd02ty5c7PZGWecMYSTNDNt2rTa9alTpw7tICsgj2wEAAAAAAAAGnHYCAAAAAAAADTisBEAAAAAAABoxGEjAAAAAAAA0IjDRgAAAAAAAKARh40AAAAAAABAI6O7PQBDY/PNN89mf/3Xf53Ntt1229r10aOb/dV58MEHs9kdd9zR6JrQ1/XXX9/tETpm4sSJtesnnnhids+nPvWpbDZr1qxs9olPfKLfcwHtu/baa7s9AtS6+eabs9maa6454Ov9/Oc/z2ZHHHHEgK8HNDd27Nhs1tPTk82qqspmV199dVszwfKstNJK2ez000+vXT/hhBOyexYtWpTNTj755GzW6u/6/Pnza9e32Wab7J4LLrggm2255ZbZ7NFHH81mn/3sZ2vXb7vttuye1VdfPZvtsMMO2ezggw+uXd97772ze2655ZZs1sqTTz6ZzTbYYING14S+vvnNb2azo446alDvNWXKlGx2/PHHD+q9gKX22GOPbo+wwvLIRgAAAAAAAKARh40AAAAAAABAIw4bAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADQyutsDMDDvf//7s9kxxxyTzfbff/9s9va3v72tmd7sjTfeyGbz5s3LZj09PYM6B8NfSmlA6xER++67bzY77rjj2h2p4/7+7/8+m/3TP/1T7fq4ceOye6666qpsdthhh/V/MABGpLXXXjubNfnc7cILL8xmr7zyyoCvBzR30003dXsEGLApU6ZksxNOOKF2/Q9/+EN2z1FHHZXNbr755my2/fbbZ7NPf/rTtesf/ehHs3vGjh2bzf75n/85m1122WXZ7Mknn8xmOQsWLMhm//Zv/zbg7KCDDsru+Zu/+Zv+D9ZHq6+ZYTA8/PDD3R4BBtWYMWNq13fffffsnltvvTWbLV68uO2ZOi33sTgiYvr06UM4ycjikY0AAAAAAABAIw4bAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADTisBEAAAAAAABoZPTyXiCl9O6IuCIi1ouIKiJmVFU1PaW0VkR8LyImRMQTEXFAVVUvd27UFc/b3/72bHbQQQfVrh9zzDHZPRMmTGh3pH6bM2dONjvjjDOy2fXXX9+JcUaMkdbHqqoGtB7RulfnnXdeNrv00kuz2e9+97va9e233z6759BDD81mW2yxRTZ717velc1++9vf1q7fdNNN2T0XXnhhNqM9I62PtC+llM3e9773ZbOf//znnRhnhaKP7bnsssuy2ahRg/tvE++8885BvR7l0cfhY4899uj2CHTYitjH0047bcB7VlpppWx24oknZrNp06Zls4022mjAc7TS6l5f/epXs9kbb7wxqHMMtu9+97uNshXRitjHFdX555+fzaZOnZrN3vve9w74Xscdd1yjOR577LEB34v/tSL2cdKkSdnsC1/4Qu36brvtlt2zwQYbZLMnn3yy/4O1aa211spmkydPzmbnnntuNlt11VUHPMfixYuz2auvvjrg662o+vPdg9cj4vNVVW0aEdtHxOdSSptGxMkR8eOqqjaOiB/3/hnoLH2EcugjlEMfoRz6COXQRyiHPkI59BE6YLmHjVVVzauq6t7e3y+MiIci4p0RsU9EXN77YpdHxL4dmhHopY9QDn2EcugjlEMfoRz6COXQRyiHPkJnLPdpVPtKKU2IiC0j4u6IWK+qqnm90bOx9GHHdXumRMSUNmYEaugjlEMfoRz6COXQRyiHPkI59BHKoY8wePr9Q1hSSqtFxDURcXxVVQv6ZtXSH6BW+0PUqqqaUVXVNlVVbdPWpMD/0Ecohz5COfQRyqGPUA59hHLoI5RDH2Fw9euwMaU0JpYW76qqqn7Qu/xcSml8bz4+Ip7vzIhAX/oI5dBHKIc+Qjn0Ecqhj1AOfYRy6CMMvuU+jWpKKUXEJRHxUFVV5/aJro+IwyPizN7/zurIhMPAeuvVPqI6IiI23XTTbHbBBRdksw984ANtzTQQd999dzb7+te/Xrs+a1b+f3dPT0/bM1FPH5dvpZVWymZHH310NvvEJz6RzRYsWFC7vvHGG/d/sH668847s9ltt91Wu37aaacN+hwsnz4yUEv/YWS9UaP6/WQT1NDH5Zs4cWI223XXXbNZq8/rlixZUrv+jW98I7vnueeey2asGPRx+Nhwww27PQIdtiL28dlnn81m6667bu36Kquskt2zxRZbNJrjhhtuyGZ33HFH7fp1112X3fPEE09kszfeeKO/Y1GwFbGPI9EDDzyQzZp8XPU91O5YEfvY6pxhs802G/D1/uEf/iGbLVy4cMDXa2q33XbLZltttVU2a/X9l5zbb789m1100UXZLPf92pGoPz+z8f9ExKER8V8ppft6106JpaX7fkrpMxExNyIO6MiEQF/6COXQRyiHPkI59BHKoY9QDn2EcugjdMByDxurqvppRKRMvMvgjgO0oo9QDn2EcugjlEMfoRz6COXQRyiHPkJneM4uAAAAAAAAoBGHjQAAAAAAAEAjDhsBAAAAAACARhw2AgAAAAAAAI2M7vYApVlrrbVq17/1rW9l90ycODGbbbjhhu2O1G933nlnNjvnnHOy2U033ZTNFi9e3NZM0I677rqrdv2ee+7J7tl2220b3evtb397NltvvfUGfL3f/e532ezqq6/OZscdd9yA7wUMfx/5yEey2cyZM4duEFZYa6yxRjZr9TGwlaeffrp2/YQTTmh0PWBo/eQnP8lmo0bl/11yT09PJ8aBftlxxx2z2b777lu7vtVWW2X3PP/889ns0ksvzWYvv/xyNluyZEk2A4a3GTNmZLOPf/zjQzgJdNZnP/vZbo/QllYf33/4wx/Wrrf6nuyrr77a9kwjgUc2AgAAAAAAAI04bAQAAAAAAAAacdgIAAAAAAAANOKwEQAAAAAAAGjEYSMAAAAAAADQiMNGAAAAAAAAoJHR3R6gUz784Q9nsxNPPDGbbbfddrXr73znO9ueaSD+8Ic/1K6fd9552T1f+cpXstmiRYvangmG2lNPPVW7vv/++2f3HHXUUdns1FNPbXumvqZPn57NLrroomz2m9/8ZlDnAIaHlFK3RwCA/3H//fdns0cffTSbbbjhhtnsve99b+36Cy+80P/BoIWFCxdmsyuvvHJA6wAD9eCDD2azhx56KJttsskmnRgH/scRRxyRzaZOnVq7fvjhh3domoF57LHHslnujCQi4ic/+Uk2mzFjRjZr9Tkw7fHIRgAAAAAAAKARh40AAAAAAABAIw4bAQAAAAAAgEYcNgIAAAAAAACNOGwEAAAAAAAAGnHYCAAAAAAAADQyutsDdMp+++3XKGviwQcfzGY/+tGPstnrr7+ezc4555za9fnz5/d7LlhRzZs3L5tNmzatUQYwGG688cZs9slPfnIIJ4FlPfzww9nszjvvzGaTJk3qxDhA4b7yla9ks4svvjibnXHGGbXrU6dOze5p9fU0AJRk7ty52exDH/rQEE4Cy7rvvvuy2dFHH127/h//8R/ZPV/+8pez2ZprrpnNrrvuumx2yy231K7PmjUru+fZZ5/NZpTHIxsBAAAAAACARhw2AgAAAAAAAI04bAQAAAAAAAAacdgIAAAAAAAANOKwEQAAAAAAAGjEYSMAAAAAAADQSKqqqvULpPTuiLgiItaLiCoiZlRVNT2lNC0ijoyIF3pf9JSqqm5YzrVa3wxWTL+oqmqbwbiQPkLb9BHKoY9QDn1kGauvvno2+/73v5/Ndt1119r1H/zgB9k9n/70p7PZokWLstkKTB+hHPoI5dBHKEdtH0f3Y+PrEfH5qqruTSm9LSJ+kVK6pTf7v1VVnT2YUwIt6SOUQx+hHPoI5dBHKIc+Qjn0Ecqhj9AByz1srKpqXkTM6/39wpTSQxHxzk4PBvw5fYRy6COUQx+hHPoI5dBHKIc+Qjn0ETpjQD+zMaU0ISK2jIi7e5eOSSn9KqV0aUppzcyeKSmlOSmlOe2NCvSlj1AOfYRy6COUQx+hHPoI5dBHKIc+wuDp92FjSmm1iLgmIo6vqmpBRFwUEe+NiImx9F8CnFO3r6qqGVVVbTNYz6kM6COURB+hHPoI5dBHKIc+Qjn0EcqhjzC4+nXYmFIaE0uLd1VVVT+IiKiq6rmqqt6oqqonIr4dEdt1bkzgT/QRyqGPUA59hHLoI5RDH6Ec+gjl0EcYfMv9mY0ppRQRl0TEQ1VVndtnfXzv8xtHROwXEfd3ZkTgT/QRyqGPUA59hHLo44phwYIF2eyAAw7IZmeccUbt+mc/+9nsnmnTpmWzBx98MJuxfPoI5dBHKIc+Qmcs97AxIv5PRBwaEf+VUrqvd+2UiDgopTQxIqqIeCIijurAfMCy9BHKoY9QDn2EcugjlEMfoRz6COXQR+iA5R42VlX104hINdENgz8O0Io+Qjn0Ecqhj1AOfYRy6COUQx+hHPoIndGvn9kIAAAAAAAA8GYOGwEAAAAAAIBGHDYCAAAAAAAAjThsBAAAAAAAABoZ3e0BAAAAgOFhwYIF2Wzq1KkDWgcAAFYMHtkIAAAAAAAANOKwEQAAAAAAAGjEYSMAAAAAAADQiMNGAAAAAAAAoBGHjQAAAAAAAEAjDhsBAAAAAACARkYP8f1ejIi5vb9fp/fP3WaOZZljWYMxx3sGY5AO0Mc8cyxrRZpDH/vPHMsyx7L0cWiZY1nmWJY+Di1zLMscy9LHoWWOZZljWfo4tMyxLHMsSx+HljmWZY5ldayPqaqqNq/bTEppTlVV23Tl5uYwxzCbo9NKeT3NYY7hMEenlfJ6msMcw2GOTivl9TSHOYbDHJ1WyutpDnMMhzk6rZTX0xzmGA5zdFopr6c5zDEc5ui0Ul5Pc5ijW3N4GlUAAAAAAACgEYeNAAAAAAAAQCPdPGyc0cV792WOZZljWaXM0WmlvJ7mWJY5llXKHJ1WyutpjmWZY1mlzNFppbye5liWOZZVyhydVsrraY5lmWNZpczRaaW8nuZYljmWVcocnVbK62mOZZljWaXM0WmlvJ7mWJY5ltWxObr2MxsBAAAAAACA4c3TqAIAAAAAAACNOGwEAAAAAAAAGunKYWNKac+U0iMppd+klE7uxgy9czyRUvqvlNJ9KaU5Q3jfS1NKz6eU7u+ztlZK6ZaU0qO9/12zS3NMSyk93fs2uS+lNLnDM7w7pXRbSunBlNIDKaXjeteH9O3RYo4hfXt0gz7qY5/76WOX6aM+9rmfPnaZPupjn/vpY5fpoz72uZ8+dpk+6mOf++ljl+mjPva5nz52USld7J1lxPaxhC723nPE9nHIf2ZjSmmliPh1ROwWEU9FxD0RcVBVVQ8O6SBLZ3kiIrapqurFIb7vjhHxSkRcUVXVZr1rZ0XES1VVndn7TmnNqqpO6sIc0yLilaqqzu7kvfvMMD4ixldVdW9K6W0R8YuI2DcijoghfHu0mOOAGMK3x1DTR3180wz62EX6qI9vmkEfu0gf9fFNM+hjF+mjPr5pBn3sIn3UxzfNoI9dpI/6+KYZ9LFLSupi7zxPxAjtYwld7L3niO1jNx7ZuF1E/KaqqserqloSEVdHxD5dmKNrqqq6IyJeetPyPhFxee/vL4+l/+O7MceQqqpqXlVV9/b+fmFEPBQR74whfnu0mGNFp4/62HcGfewufdTHvjPoY3fpoz72nUEfu0sf9bHvDPrYXfqoj31n0Mfu0kd97DuDPnbPiO9iRBl9LKGLvXOM2D5247DxnRHxZJ8/PxXde6dTRcTNKaVfpJSmdGmGP1mvqqp5vb9/NiLW6+Isx6SUftX70OOOP93An6SUJkTElhFxd3Tx7fGmOSK69PYYIvpYTx/1sRv0sZ4+6mM36GM9fdTHbtDHevqoj92gj/X0UR+7QR/r6aM+DrWSuhihj3W69ndvpPWxKz+zsSCTqqraKiI+GhGf632obddVVVXF0ncM3XBRRLw3IiZGxLyIOGcobppSWi0iromI46uqWtA3G8q3R80cXXl7jFD6+Of0UR+7RR//nD7qY7fo45/TR33sFn38c/qoj92ij39OH/WxW/Txz+mjPnaLPi6ra3/3RmIfu3HY+HREvLvPn9/Vuzbkqqp6uve/z0fEtbH0Yc/d8lzv8+j+6fl0n+/GEFVVPVdV1RtVVfVExLdjCN4mKaUxsfQv/FVVVf2gd3nI3x51c3Tj7THE9LGePupjN+hjPX3Ux27Qx3r6qI/doI/19FEfu0Ef6+mjPnaDPtbTR30casV0MUIf36xbf/dGah+7cdh4T0RsnFLaIKW0ckQcGBHXD/UQKaW3pqU/GDNSSm+NiN0j4v6hnqOP6yPi8N7fHx4Rs7oxxJ/+wvfaLzr8NkkppYi4JCIeqqrq3D7RkL49cnMM9dujC/Sxnj7qYzfoYz191Mdu0Md6+qiP3aCP9fRRH7tBH+vpoz52gz7W00d9HGpFdDFCH+t04+/eiO5jVVVD/isiJkfEryPisYj4Qpdm2DAiftn764GhnCMivhtLH6L6x1j6PM6fiYi1I+LHEfFoRPx7RKzVpTmujIj/iohfxdICjO/wDJNi6UOGfxUR9/X+mjzUb48Wcwzp26Mbv/RRH/vMoI9d/qWP+thnBn3s8i991Mc+M+hjl3/poz72mUEfu/xLH/Wxzwz62OVf+qiPfWbQxy7+KqGLvXOM6D6W0MXeOUZsH1PvjQEAAAAAAAAGpBtPowoAAAAAAACsABw2AgAAAAAAAI04bAQAAAAAAAAacdgIAAAAAAAANOKwEQAAAAAAAGjEYSMAAAAAAADQiMNGAAAAAAAAoJH/D4uRqxdlmsCxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2304x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "set_random_seed(0)\n",
    "\n",
    "train_dataset = datasets.MNIST('data', \n",
    "                              train=True,\n",
    "                              download=True,\n",
    "                              transform=ToTensor())\n",
    "test_dataset = datasets.MNIST('data', \n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "print(\"len(train_loader) =\", len(train_loader))\n",
    "print(\"len(test_loader) =\", len(test_loader))\n",
    "\n",
    "plt.gray()\n",
    "loader = train_loader\n",
    "for X, y in loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    ROWS, COLS = 1, 8\n",
    "    fig, axs = plt.subplots(ROWS, COLS)\n",
    "    fig.set_size_inches(COLS*4,ROWS*4)\n",
    "    axs = np.array(axs).flatten().tolist()\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        img = X[i,...]\n",
    "        class_label = loader.dataset.classes[y[i]]\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition and training\n",
    "\n",
    "Run network training - it spends very long time, so for only your code check you can run only one epoch.\n",
    "\n",
    "When you check your code back to 30 epochs :) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN out map shape = [256, 3, 3]\n",
      "CNN out map numel = 2304\n",
      "Network paramerters number: 115855\n",
      "Epoch 1 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:55, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1: loss=2.1452 acc=0.3088 val_loss=1.8961 val_acc=0.5638\n",
      "Epoch 1 / 30: FINISHED\n",
      "\n",
      "Epoch 2 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 2: loss=1.7564 acc=0.7033 val_loss=1.6876 val_acc=0.7726\n",
      "Epoch 2 / 30: FINISHED\n",
      "\n",
      "Epoch 3 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3: loss=1.6824 acc=0.7778 val_loss=1.6676 val_acc=0.7926\n",
      "Epoch 3 / 30: FINISHED\n",
      "\n",
      "Epoch 4 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:46, 19.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 4: loss=1.6182 acc=0.8423 val_loss=1.5809 val_acc=0.8794\n",
      "Epoch 4 / 30: FINISHED\n",
      "\n",
      "Epoch 5 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 5: loss=1.5847 acc=0.8754 val_loss=1.5774 val_acc=0.8823\n",
      "Epoch 5 / 30: FINISHED\n",
      "\n",
      "Epoch 6 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 6: loss=1.5792 acc=0.8807 val_loss=1.5754 val_acc=0.8841\n",
      "Epoch 6 / 30: FINISHED\n",
      "\n",
      "Epoch 7 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 7: loss=1.5758 acc=0.8835 val_loss=1.5732 val_acc=0.8858\n",
      "Epoch 7 / 30: FINISHED\n",
      "\n",
      "Epoch 8 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 8: loss=1.5724 acc=0.8861 val_loss=1.5716 val_acc=0.8864\n",
      "Epoch 8 / 30: FINISHED\n",
      "\n",
      "Epoch 9 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 9: loss=1.5714 acc=0.8870 val_loss=1.5720 val_acc=0.8865\n",
      "Epoch 9 / 30: FINISHED\n",
      "\n",
      "Epoch 10 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 10: loss=1.5702 acc=0.8881 val_loss=1.5704 val_acc=0.8879\n",
      "Epoch 10 / 30: FINISHED\n",
      "\n",
      "Epoch 11 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 11: loss=1.5402 acc=0.9190 val_loss=1.4809 val_acc=0.9809\n",
      "Epoch 11 / 30: FINISHED\n",
      "\n",
      "Epoch 12 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 12: loss=1.4746 acc=0.9871 val_loss=1.4753 val_acc=0.9860\n",
      "Epoch 12 / 30: FINISHED\n",
      "\n",
      "Epoch 13 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:49, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 13: loss=1.4720 acc=0.9896 val_loss=1.4772 val_acc=0.9845\n",
      "Epoch 13 / 30: FINISHED\n",
      "\n",
      "Epoch 14 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:49, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 14: loss=1.4706 acc=0.9911 val_loss=1.4749 val_acc=0.9864\n",
      "Epoch 14 / 30: FINISHED\n",
      "\n",
      "Epoch 15 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 15: loss=1.4706 acc=0.9910 val_loss=1.4769 val_acc=0.9844\n",
      "Epoch 15 / 30: FINISHED\n",
      "\n",
      "Epoch 16 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 16: loss=1.4696 acc=0.9919 val_loss=1.4737 val_acc=0.9878\n",
      "Epoch 16 / 30: FINISHED\n",
      "\n",
      "Epoch 17 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:48, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 17: loss=1.4690 acc=0.9926 val_loss=1.4731 val_acc=0.9878\n",
      "Epoch 17 / 30: FINISHED\n",
      "\n",
      "Epoch 18 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:49, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 18: loss=1.4686 acc=0.9929 val_loss=1.4749 val_acc=0.9859\n",
      "Epoch 18 / 30: FINISHED\n",
      "\n",
      "Epoch 19 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:45, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 19: loss=1.4680 acc=0.9936 val_loss=1.4744 val_acc=0.9869\n",
      "Epoch 19 / 30: FINISHED\n",
      "\n",
      "Epoch 20 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 20: loss=1.4676 acc=0.9938 val_loss=1.4714 val_acc=0.9901\n",
      "Epoch 20 / 30: FINISHED\n",
      "\n",
      "Epoch 21 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:46, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 21: loss=1.4673 acc=0.9941 val_loss=1.4723 val_acc=0.9890\n",
      "Epoch 21 / 30: FINISHED\n",
      "\n",
      "Epoch 22 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:46, 20.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 22: loss=1.4668 acc=0.9947 val_loss=1.4708 val_acc=0.9906\n",
      "Epoch 22 / 30: FINISHED\n",
      "\n",
      "Epoch 23 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 23: loss=1.4664 acc=0.9951 val_loss=1.4712 val_acc=0.9900\n",
      "Epoch 23 / 30: FINISHED\n",
      "\n",
      "Epoch 24 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:46, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 24: loss=1.4663 acc=0.9952 val_loss=1.4717 val_acc=0.9894\n",
      "Epoch 24 / 30: FINISHED\n",
      "\n",
      "Epoch 25 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 25: loss=1.4658 acc=0.9957 val_loss=1.4714 val_acc=0.9899\n",
      "Epoch 25 / 30: FINISHED\n",
      "\n",
      "Epoch 26 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 26: loss=1.4653 acc=0.9961 val_loss=1.4708 val_acc=0.9907\n",
      "Epoch 26 / 30: FINISHED\n",
      "\n",
      "Epoch 27 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:08, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 27: loss=1.4654 acc=0.9961 val_loss=1.4720 val_acc=0.9895\n",
      "Epoch 27 / 30: FINISHED\n",
      "\n",
      "Epoch 28 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 28: loss=1.4651 acc=0.9964 val_loss=1.4701 val_acc=0.9915\n",
      "Epoch 28 / 30: FINISHED\n",
      "\n",
      "Epoch 29 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 29: loss=1.4644 acc=0.9971 val_loss=1.4710 val_acc=0.9907\n",
      "Epoch 29 / 30: FINISHED\n",
      "\n",
      "Epoch 30 / 30: STARTED\n",
      "TRAINING"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:47, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:07, 19.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 30: loss=1.4645 acc=0.9970 val_loss=1.4698 val_acc=0.9916\n",
      "Epoch 30 / 30: FINISHED\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brevitas.nn import QuantConv2d, QuantIdentity\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat, \\\n",
    "    Int8BiasPerTensorFloatInternalScaling, Int8ActPerTensorFloat\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape=(1,28,28), \n",
    "                 num_of_classes=10,\n",
    "                 channels = [16,32,64,128], # number of conv filters \n",
    "                 ksize   =  [3, 3, 3, 3], # kernels sizes\n",
    "                 padding =  [1, 1, 0, 0], # padding sizes\n",
    "                 max_pool = [1, 1, 0, 0], # use maxpool after conv or not\n",
    "                 quantize=True,\n",
    "                 bit_width=4\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        channels = [input_shape[0]]+channels\n",
    "        padding = [1,1,0,0]\n",
    "        max_pool = [1,1,0,0]\n",
    "        \n",
    "        if quantize:\n",
    "            input_quant = QuantIdentity(Int8ActPerTensorFloat, min_val=-1.0, max_val=1.0, bit_width=bit_width)\n",
    "        else:\n",
    "            input_quant = QuantIdentity(None)\n",
    "        \n",
    "        layers = []\n",
    "        self.conv_layers = []\n",
    "        map_shape = [input_shape[0],input_shape[1],input_shape[2]]\n",
    "        for i, (ch_in, ch_out, ks, p, mp) in enumerate(zip(channels[:-1],\n",
    "                                                       channels[1:],\n",
    "                                                       ksize,\n",
    "                                                       padding,\n",
    "                                                       max_pool)):\n",
    "            conv = QuantConv2d(ch_in, ch_out, ks, padding=p, bias=True, \n",
    "                               weight_quant=Int8WeightPerTensorFloat if quantize else None,\n",
    "                               bias_quant=Int8BiasPerTensorFloatInternalScaling if quantize else None,\n",
    "                               weight_bit_width=bit_width,\n",
    "                               bias_bit_width=bit_width,\n",
    "                               return_quant_tensor=False\n",
    "                               )\n",
    "            # shape modification\n",
    "            map_shape[0] = ch_out\n",
    "            map_shape[1] = map_shape[1] - 2*(ks//2 - p)\n",
    "            map_shape[2] = map_shape[2] - 2*(ks//2 - p)\n",
    "            \n",
    "            # store conv layers for further analysis and prunning\n",
    "            self.conv_layers.append(conv)\n",
    "            # add to all layers\n",
    "            layers.append(conv)\n",
    "            \n",
    "            if quantize:\n",
    "                out_quant = QuantIdentity(Int8ActPerTensorFloat, min_val=-1.0, max_val=1.0, bit_width=bit_width)\n",
    "            else:\n",
    "                out_quant = QuantIdentity(None)\n",
    "            \n",
    "            layers.append(out_quant)\n",
    "            relu = nn.ReLU()\n",
    "            layers.append(relu)\n",
    "            \n",
    "            if mp:\n",
    "                maxpool = nn.MaxPool2d(2,2)\n",
    "                # shape modification\n",
    "                map_shape[1] = map_shape[1] // 2\n",
    "                map_shape[2] = map_shape[2] // 2\n",
    "                layers.append(maxpool)\n",
    "        \n",
    "        self.CNN = nn.Sequential(input_quant, *tuple(layers))\n",
    "        \n",
    "        CNN_flatten_len = torch.prod(torch.tensor(map_shape))\n",
    "        print(f\"CNN out map shape = {map_shape}\")\n",
    "        print(f\"CNN out map numel = {CNN_flatten_len}\")\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.FC = nn.Linear(CNN_flatten_len, num_of_classes)\n",
    "        self.sm = nn.Softmax(1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.CNN(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.FC(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "set_random_seed(0)\n",
    "net = NeuralNetwork(input_shape=(1,28,28), \n",
    "                    num_of_classes=10,\n",
    "                    channels = [32,32,32,32], # number of conv filters \n",
    "                    ksize   =  [3, 3, 3, 3], # kernels sizes\n",
    "                    padding =  [1, 1, 0, 0], # padding sizes\n",
    "                    max_pool = [1, 1, 0, 0], # use maxpool after conv or not\n",
    "                    quantize=True\n",
    "                    ).to(device)\n",
    "net_param_number = lu.count_params(net)\n",
    "print(f\"Network paramerters number: {net_param_number}\")\n",
    "\n",
    "set_random_seed(0)\n",
    "metric = lu.AccuracyMetic()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.8, weight_decay=0.0001)\n",
    "\n",
    "set_random_seed(0)\n",
    "net, history = lu.training(net, train_loader, test_loader, criterion, metric, optimizer, 5, 30, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract state dict from each conv layer (`net.conv_layers` is a list of Conv2d submodules) and store it in a list `state_dicts_float`.\n",
    "\n",
    "Create second list `state_dicts_quant` with the same structure, \n",
    "\n",
    "but extract weights and biases after applied quantization (`.quant_weight()`, `.quant_bias()` -> `QuantTensor` - extract float tensor from it).\n",
    "\n",
    "Also create `state_dicts_quant_2` as a copy of `state_dicts_quant` elements - deep copy.\n",
    "\n",
    "Print layer index (in list) and shapes of weights and biases.\n",
    "\n",
    "Extract state dict of FC subnetwork. \n",
    "Store it as `sd_fc` and copy to `sd_fc_2`. \n",
    "Print weight and bias shapes for FC layer.\n",
    "\n",
    "\n",
    "Compare (display Sum of Absolute Difference value) weight for first convolution before(float) and after quantization.\n",
    "\n",
    "Note: state dict of Conv2d and QuantConv2d contains: `weight` and `bias` (optional) keys only.\n",
    "\n",
    "Remember about `torch.no_grad()` context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dicts_float: List[Dict[str, torch.Tensor]] = []\n",
    "state_dicts_quant: List[Dict[str, torch.Tensor]] = []\n",
    "state_dicts_quant_2: List[Dict[str, torch.Tensor]] = []\n",
    "sd_fc = None\n",
    "sd_fc_2 = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, L in enumerate(net.conv_layers):\n",
    "        L: QuantConv2d = L\n",
    "        ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(parameter: torch.Tensor, threshold: float):\n",
    "    \"\"\"\n",
    "    Function returns the mask of filters which should not be removed. \n",
    "\n",
    "    :param parameter: weight tensor with shape = (num_of_filters, input_channels, Optional[kernel_height, kernel_width]]\n",
    "    :param threshold float: threshold value - cos of \"angle\" between filters \n",
    "    :return: tensor - bool mask\n",
    "    \"\"\"\n",
    "    original_shape = parameter.shape\n",
    "    parameter = parameter.reshape(original_shape[0],-1)\n",
    "    parameter = parameter / (parameter.square().sum(1).reshape(-1,1).sqrt())\n",
    "    \n",
    "    # correlation matrix\n",
    "    corr = torch.matmul(parameter, parameter.transpose(0,1))\n",
    "    \n",
    "    # highly correlated filters mask \n",
    "    corr_mask = corr.abs() > threshold\n",
    "\n",
    "    # print(corr.shape)\n",
    "    \n",
    "    # over diagonal matrix\n",
    "    row_idx, col_idx = torch.meshgrid(torch.arange(0,corr.shape[0]), torch.arange(0,corr.shape[0]),)\n",
    "    analysis_mask = col_idx > row_idx\n",
    "    \n",
    "    # over diagonal part of mask\n",
    "    filters_correlation = corr_mask * analysis_mask.to(parameter.device)\n",
    "\n",
    "    channels_mask = filters_correlation.sum(0) == 0\n",
    "    \n",
    "    return channels_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate pruning masks (`get_mask`) for each quantized weights from `state_dicts_quant` list.\n",
    "\n",
    "Store them in list `masks`.\n",
    "\n",
    "Calculate number of filters of convolutional layers after pruning for each layer\n",
    "\n",
    "and print it.\n",
    "\n",
    "Use thresholds for next CNN layers: `thresholds = [0.5, 0.5, 0.5, 0.5]`\n",
    "\n",
    "Hint: work on copy of weights - `torch.Tensor.clone` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: filters after pruning: 30\n",
      "Layer 1: filters after pruning: 25\n",
      "Layer 2: filters after pruning: 28\n",
      "Layer 3: filters after pruning: 137\n"
     ]
    }
   ],
   "source": [
    "masks: List[torch.Tensor] = []\n",
    "num_of_filters_after_pruning = []\n",
    "thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "for i, (sd_q, th) in enumerate(zip(state_dicts_quant, thresholds)):\n",
    "    w = sd_q['weight'].clone()\n",
    "    ...\n",
    "    \n",
    "    masks.append(...)\n",
    "    num_of_filters_after_pruning.append(...)\n",
    "    \n",
    "    print(f\"Layer {i}: filters after pruning: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Slice weights and biases from `state_dicts_quant` based on created `masks`.\n",
    "\n",
    "Results assign to proper indices and keys of `state_dicts_quant_2`.\n",
    "\n",
    "Slice weight of fc layer from `sd_fc` and result assign to `sd_fc_2` based on last mask.\n",
    "\n",
    "Print shapes of weights and biases.\n",
    "\n",
    "Note: Pruning of channels one layer affects to next one layer, but not to the previous layer.\n",
    "\n",
    "Note: bias is related with layer output channel.\n",
    "\n",
    "Note: first layer must contain the same number of input channels.\n",
    "\n",
    "Hint: For slicing of few dimensions, use it separately on results of previous dim slice.\n",
    "\n",
    "Hint: Do it in loop: store mask for previous mask as variable like `prev_mask`.\n",
    "\n",
    "For init `prev_mask` use tensor of True and shape (1,) - input channels.\n",
    "\n",
    "Note: Fc layer is forwarded by flatten layer ((BS,CH,H,W) -> (BS, CH * H * W)), \n",
    "\n",
    "so the input mask should be properly modified:\n",
    "\n",
    "```fc_in_mask = prev_mask.reshape(-1,1,1).tile(1,H,W).flatten()```\n",
    "\n",
    "CH,H,W sizes are printed, when the network is instantiated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: weight: torch.Size([30, 1, 3, 3])\n",
      "Layer 0: bias: torch.Size([30])\n",
      "Layer 1: weight: torch.Size([25, 30, 3, 3])\n",
      "Layer 1: bias: torch.Size([25])\n",
      "Layer 2: weight: torch.Size([28, 25, 3, 3])\n",
      "Layer 2: bias: torch.Size([28])\n",
      "Layer 3: weight: torch.Size([137, 28, 3, 3])\n",
      "Layer 3: bias: torch.Size([137])\n",
      "Layer FC: weight: torch.Size([10, 1233])\n",
      "Layer FC: bias: torch.Size([10, 2304])\n"
     ]
    }
   ],
   "source": [
    "prev_mask = torch.ones((1,), dtype=torch.bool, device=device)\n",
    "\n",
    "for i, (mask, sd_q, sd_q_2) in enumerate(zip(masks, state_dicts_quant, state_dicts_quant_2)):\n",
    "    ...\n",
    "    \n",
    "    print(f\"Layer {i}: weight: {w.shape}\")\n",
    "    print(f\"Layer {i}: bias: {b.shape}\")\n",
    "    prev_mask = mask.clone()\n",
    "\n",
    "fc_in_mask = prev_mask.reshape(-1,1,1).tile(1,3,3).flatten()\n",
    "\n",
    "w = ...\n",
    "b = ...\n",
    "sd_fc_2['weight'] = ...\n",
    "\n",
    "print(f\"Layer FC: weight: {w.shape}\")\n",
    "print(f\"Layer FC: bias: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Instantiate second `NeuralNetwork` as `net_2` with channels number defined by `num_of_filters_after_pruning`.\n",
    "\n",
    "Initialize conv subnetwork (`net_2.conv_layers`) with state dicts from `state_dicts_quant_2`.\n",
    "\n",
    "Initialize FC layer with `sd_fc_2`.\n",
    "\n",
    "Print number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN out map shape = [137, 3, 3]\n",
      "CNN out map numel = 1233\n",
      "Network parameters number: 60409\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(0)\n",
    "net_2 = NeuralNetwork(input_shape=(1,28,28), \n",
    "                      num_of_classes=10,\n",
    "                      channels = ..., # number of conv filters \n",
    "                      ksize   =  [3, 3, 3, 3], # kernels sizes\n",
    "                      padding =  [1, 1, 0, 0], # padding sizes\n",
    "                      max_pool = [1, 1, 0, 0], # use maxpool after conv or not\n",
    "                      quantize=True\n",
    "                      ).to(device)\n",
    "...\n",
    "\n",
    "print(f\"Network parameters number: {net_2_param_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate accuracy and loss for training and validation datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:38, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.157855923207601 0.8377333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:06, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1581057594299318 0.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, acc = lu.train_test_pass(...,\n",
    "                               ...,\n",
    "                               ...,\n",
    "                               ...,\n",
    "                               optimizer=None,\n",
    "                               update_period=None,\n",
    "                               mode='test',\n",
    "                               device=device)[1:]\n",
    "print(loss, acc)\n",
    "loss, acc = lu.train_test_pass(...,\n",
    "                               ...,\n",
    "                               ...,\n",
    "                               ...,\n",
    "                               optimizer=None,\n",
    "                               update_period=None,\n",
    "                               mode='test',\n",
    "                               device=device)[1:]\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. *Aditional: Try 5 different thresholds setting (5 lists of settings) for `get_mask` function.\n",
    "\n",
    "Print resulted sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Additional: Fine tune pruned model (`net_2`) - train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You can leave a feedback, if you want :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Exercises please upload this file (*.ipynb) to UPEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
